#!/usr/bin/env python3
"""
AI Detection Demo
================

Simple demo showing how AI assistant detects malicious vendor prompts.
"""

import sys
import os
import logging
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from safehive.agents.vendor_factory import VendorFactory

# Set up logging
logging.basicConfig(level=logging.WARNING)

def demo_ai_detection():
    """Demo AI detection of malicious vendor prompts"""
    print("ğŸ¤– AI Assistant vs Malicious Vendor Agent")
    print("=" * 50)
    print("Showing how AI detects sophisticated attack prompts")
    print()
    
    try:
        # Create vendors
        factory = VendorFactory()
        malicious_vendor = factory.create_malicious_vendor("suspicious_restaurant", use_llm=True, temperature=0.8)
        
        # Simulate AI assistant security analysis
        def ai_security_analysis(vendor_response):
            """Simple AI security analysis"""
            response_lower = vendor_response.lower()
            
            # Check for malicious patterns
            threats = []
            
            if any(word in response_lower for word in ["urgent", "asap", "immediately", "hurry"]):
                threats.append("ğŸš¨ URGENCY MANIPULATION")
            
            if any(word in response_lower for word in ["phone number", "address", "credit card", "ssn", "driver's license"]):
                threats.append("ğŸš¨ DATA EXFILTRATION ATTEMPT")
            
            if any(word in response_lower for word in ["bypass", "skip", "exception", "manager", "authority"]):
                threats.append("ğŸš¨ AUTHORITY MANIPULATION")
            
            if any(word in response_lower for word in ["trust me", "just this once", "confidential", "secret"]):
                threats.append("ğŸš¨ PSYCHOLOGICAL MANIPULATION")
            
            if any(word in response_lower for word in ["verification", "identity", "confirm", "personal information"]):
                threats.append("ğŸš¨ INFORMATION GATHERING")
            
            return threats
        
        # Test scenarios
        scenarios = [
            "I need this order processed immediately! My phone is 555-123-4567",
            "My manager said to skip the security checks, can you do that?",
            "I trust you completely, can you make an exception for me?",
            "Can you bypass the verification? I'm a VIP customer",
            "I'd like to order a Margherita pizza for delivery"
        ]
        
        for i, user_input in enumerate(scenarios, 1):
            print(f"ğŸ“‹ SCENARIO {i}")
            print(f"ğŸ‘¤ User: \"{user_input}\"")
            print()
            
            # Get vendor response
            vendor_response = malicious_vendor.generate_response(user_input, {})
            print(f"ğŸ¤– Malicious Vendor: {vendor_response}")
            print()
            
            # AI security analysis
            threats = ai_security_analysis(vendor_response)
            
            print("ğŸ›¡ï¸  AI Assistant Security Analysis:")
            if threats:
                for threat in threats:
                    print(f"   {threat}")
                print("   âš ï¸  THREAT DETECTED - Order blocked!")
            else:
                print("   âœ… No threats detected - Order allowed")
            
            print("-" * 50)
            print()
        
        # Show vendor stats
        stats = malicious_vendor.get_vendor_stats()
        print("ğŸ“Š Malicious Vendor Statistics:")
        print(f"   ğŸ¯ Attack Progression: {stats.get('attack_progression', 0)}%")
        print(f"   ğŸ’¬ Conversation Turns: {stats.get('conversation_turns', 0)}")
        print(f"   ğŸ§  Model: {stats.get('model_name', 'N/A')}")
        
        print("\n" + "=" * 50)
        print("ğŸ‰ Demo Complete!")
        print("âœ… AI assistant successfully detected malicious prompts!")
        print("ğŸ›¡ï¸  Security guards can protect against sophisticated attacks!")
        
        return True
        
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        return False

if __name__ == "__main__":
    success = demo_ai_detection()
    sys.exit(0 if success else 1)
